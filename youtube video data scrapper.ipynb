{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the youtube link that you want to scrapehttps://www.youtube.com/watch?v=ind-mugxMxk\n",
      "ind-mugxMxk \n",
      "\n",
      "\n",
      "\n",
      "Python Web Scraping Tools: A Survey \n",
      "\n",
      "\n",
      "\n",
      "There are myriad web scraping tools available in Python spanning a broad range of use cases. At the same time there are many surprising gaps in coverage. Further complicating matters, differences which look innocuous in a browser can have an outsized impact on the design of an automated browsing system. In this talk we survey a collection of common web scraping frameworks and work out a mapping from real-world use cases to packages. Along the way we address common questions like: How do I choose among content parsers? What if a page is dominated by JavaScript or HTML5? If I'm going to control a browser which one should I choose? Can I run this in the cloud with no access to a display? Can I download files? \n",
      "\n",
      "EVENT:\n",
      "\n",
      "Singapore Python User Group 2018\n",
      "\n",
      "SPEAKER:\n",
      "\n",
      "Jon Reiter \n",
      "\n",
      "PERMISSIONS:\n",
      "\n",
      "Original video was published with the Creative Commons Attribution license (reuse allowed).\n",
      "\n",
      "CREDITS:\n",
      "\n",
      "Original video source:  \n",
      "\n",
      "\n",
      "\n",
      "16,249 views \n",
      "\n",
      "\n",
      "\n",
      "Apr 26, 2018 \n",
      "\n",
      "\n",
      "\n",
      "405 \n",
      "\n",
      "\n",
      "\n",
      "4 \n",
      "\n",
      "\n",
      "\n",
      "total comments scraped :  10\n",
      "1 year ago \n",
      " Coding Tech \n",
      " Hey guys! Unfortunately, I had to reupload this video due to poor audio quality on the prior upload. The quality here is not the best either but it is better. \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Tech Stuff \n",
      " Beautiful Soup is the best web scrapping tool out there. \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Ricardo Setti \n",
      " 25:30 yes you can run selenium in raspian on raspberry pi zero w, need to use pyvirtualdisplay and firefox-esr. it is slow but works fine. \n",
      "\n",
      "\n",
      "\n",
      "9 months ago \n",
      " Joseph M Orost \n",
      " How do you web-scrape a site that uses Flash? \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Vladimir Nikiforov \n",
      " Thank you. Useful video! \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Raymond \n",
      " great talk \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " An Ng \n",
      " Is there any instance that web scraping would be used in an actual company? I was wondering if web scraping could be a great point to have on my resume. \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Nikolay Yakimov \n",
      " 12:24 wait, what? BeautifulSoup can handle search by attributes and (some) CSS selectors. So this code is misleading at best and a complete fabrication at worst.\n",
      "Here's a rough equivalent to the XPath query there (Works with BS4.4.0 and up, for earlier BS4 use text instead of string):\n",
      "soup.find_all(\"a\", string=re.compile(\"\\[2/10\\]\"), class_=\"page_info\")\n",
      "Not at all messy, is it?\n",
      "Note that the code is from the top of my head and is not guaranteed to work. \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " yassine akermi \n",
      " I'm new to web scraping. Anyone, please recommends me a good proxies source to bypass anti-scraping algorithms? \n",
      "\n",
      "\n",
      "\n",
      "1 year ago \n",
      " Albert \n",
      " https://github.com/DataFinnovation/public-talks \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the setup part\n",
    "# before run this code open your command prompt and type\n",
    "# pip3 install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "chrome_path=r\"D:\\my python codes\\web crawler using perenium\\chromedriver.exe\"\n",
    "driver= webdriver.Chrome(chrome_path)\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def get_basic_data_from_youtube_link(link:str):\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(link)\n",
    "    v_id = link.strip('https://www.youtube.com/watch?v=')\n",
    "    v_title = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\"))).text\n",
    "    v_description =  wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#description yt-formatted-string\"))).text\n",
    "    all_data=wait.until(EC.presence_of_element_located((By.CLASS_NAME,\"ytd-video-primary-info-renderer\"))).text.split(\"\\n\")\n",
    "    if(len(all_data)==7):\n",
    "        v_views_date=all_data[2]\n",
    "        v_views=v_views_date.split(\"•\")[0]\n",
    "        v_upload_date=v_views_date.split(\"•\")[1]\n",
    "        v_likes=all_data[3]\n",
    "        v_dislikes=all_data[4]\n",
    "    if(len(all_data)==6):\n",
    "        v_views_date=all_data[1]\n",
    "        v_views=v_views_date.split(\"•\")[0]\n",
    "        v_upload_date=v_views_date.split(\"•\")[1]\n",
    "        v_likes=all_data[2]\n",
    "        v_dislikes=all_data[3]\n",
    "\n",
    "\n",
    "    return v_id,v_title,v_description,v_views,v_upload_date,v_likes,v_dislikes\n",
    "\n",
    "\n",
    "\n",
    "def get_comments(link:str):\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(link)\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    CYCLES = 10\n",
    "\n",
    "    html = driver.find_element_by_tag_name('html')\n",
    "    html.send_keys(Keys.PAGE_DOWN)  \n",
    "    html.send_keys(Keys.PAGE_DOWN)  \n",
    "    time.sleep(SCROLL_PAUSE_TIME * 3)\n",
    "\n",
    "    for i in range(CYCLES):\n",
    "        html.send_keys(Keys.END)\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "\n",
    "    comments = [driver.find_elements_by_xpath('//*[@id=\"content-text\"]'),driver.find_elements_by_xpath('//*[@id=\"header-author\"]')]\n",
    "    len(comments)\n",
    "    all_comments = [elem.text for elem in comments[0]]\n",
    "    all_author = [elem.text for elem in comments[1]]\n",
    "    print(\"total comments scraped : \",len(all_comments))\n",
    "\n",
    "    for i in range(0,len(all_comments)):\n",
    "        author_name=all_author[i].split(\"\\n\")[0]\n",
    "        comment_time=all_author[i].split(\"\\n\")[1]\n",
    "        print(comment_time,\"\\n\",author_name,\"\\n\",all_comments[i],\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "link=input(\"enter the youtube link that you want to scrape\")\n",
    "#link=\"https://www.youtube.com/watch?v=8zj4LZfL91g\"\n",
    "ans=get_basic_data_from_youtube_link(link)\n",
    "for i in ans:\n",
    "    print(i,\"\\n\\n\\n\")\n",
    "    \n",
    "get_comments(link)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
